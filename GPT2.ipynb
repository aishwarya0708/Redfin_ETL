{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBbW35RSV0lL5FSzMDgkHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishwarya0708/Redfin_ETL/blob/main/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "beNAtmGI_dv7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization,Dropout\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self Attention**- to understand how words are related to eachother"
      ],
      "metadata": {
        "id": "Cg7FqBF6A6LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(Layer):\n",
        "\n",
        "   def __init__(self, embed_dim, num_heads):\n",
        "     super().__init__()\n",
        "     self.embed_dim=embed_dim\n",
        "     self.num_heads=num_heads\n",
        "     self.attn_head_size=embed_dim//num_heads\n",
        "     self.query_dense=Dense(embed_dim)\n",
        "     self.key_dense=Dense(embed_dim)\n",
        "     self.value_dense=Dense(embed_dim)\n",
        "\n",
        "     self.dense=Dense(embed_dim)\n",
        "   #single head has to be split into 12 heads\n",
        "   def split_heads(self,x,batch_size):\n",
        "      x=tf.reshape(x,(batch_size,-1,self.num_heads,self.attn_head_size))\n",
        "      return tf.transpose(x,perm=[0,2,1,3])\n",
        "\n",
        "   def call(self,query_dense,key_dense,value_dense, mask=None): # Added mask=None\n",
        "      batch_size=tf.shape(query_dense)[0]\n",
        "      query=self.query_dense(query_dense)\n",
        "      key=self.key_dense(key_dense)\n",
        "      value=self.value_dense(value_dense)\n",
        "\n",
        "      matmul_qk=tf.matmul(query,key,transpose_b=True) # Changed query_dense to query\n",
        "      dk=tf.cast(tf.shape(key)[-1],tf.float32) # Changed key_dense to key\n",
        "      scaled_attention_logits=matmul_qk/tf.math.sqrt(dk)\n",
        "#scores of words without a relationship are changed to 0\n",
        "      if mask is not None:\n",
        "        scaled_attention_logits+=(mask*-1e9)\n",
        "\n",
        "      attention_weights=tf.nn.softmax(scaled_attention_logits,axis=-1)\n",
        "      output=tf.matmul(attention_weights,value)\n",
        "      #original shape\n",
        "      concat_attention=tf.reshape(output,shape=(batch_size,-1,self.embed_dim))\n",
        "      return self.dense(concat_attention)"
      ],
      "metadata": {
        "id": "8YcLszTMBEX-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward Neural Network\n"
      ],
      "metadata": {
        "id": "izmxkL1FnLH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(Layer):\n",
        "    def __init__(self, embed_dim, dff):\n",
        "        super().__init__()\n",
        "        self.dense1 = Dense(dff, activation='gelu')\n",
        "        self.dense2 = Dense(embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.dense2(self.dense1(x))"
      ],
      "metadata": {
        "id": "WB_1aeZcnSAH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement teansformer block from Layer\n"
      ],
      "metadata": {
        "id": "jGPLR8ZYqSfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = FeedForwardNetwork(embed_dim, dff)\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        attn_output = self.att(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.norm2(out1 + ffn_output)\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = FeedForwardNetwork(embed_dim, dff)\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        attn_output = self.att(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.norm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "eWUGAV1OqWJ3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(Model):\n",
        "  def __init__(self,vocab_size,max_length,embed_dim=768,num_heads=12,dff=3072,num_layers=12,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.token_emb=Embedding(vocab_size,embed_dim)\n",
        "    self.pos_emb=Embedding(vocab_size,embed_dim)\n",
        "    self.embedding=Embedding(vocab_size,embed_dim)\n",
        "    self.transformer_blocks=tf.keras.Sequential([TransformerBlock(embed_dim,num_heads,dff,dropout_rate) for num in range(num_layers)])\n",
        "    self.norm=LayerNormalization(epsilon=1e-6)\n",
        "    self.out=Dense(vocab_size)\n",
        "\n",
        "    #create mask\n",
        "  def create_casual_mask(self,seq_len):\n",
        "    mask=tf.linalg.band_part(tf.ones((seq_len,seq_len)),-1,0)\n",
        "    return 1-mask\n",
        "\n",
        "  def call(self,x):\n",
        "    seq_len=tf.shape(x)[1]\n",
        "    casual_mask=self.create_casual_mask(seq_len)\n",
        "    token_embeddings=self.token_emb(x)\n",
        "    pos_embeddings=self.pos_emb(tf.range(start=0,limit=seq_len,delta=1))\n",
        "    embeddings=token_embeddings+pos_embeddings\n",
        "\n",
        "    #runs 12 times\n",
        "    # Iterate through the layers in transformer_blocks using .layers\n",
        "    for transformer in self.transformer_blocks.layers:\n",
        "      embeddings=transformer(embeddings,casual_mask)\n",
        "    # Removed extra call to self.out\n",
        "    return self.out(embeddings) # Only call self.out once"
      ],
      "metadata": {
        "id": "XmN-SEMAtXQk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE=50257\n",
        "MAX_LENGTH=1824\n",
        "inputs = tf.keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32)\n",
        "outputs = GPT2(vocab_size=VOCAB_SIZE, max_length=MAX_LENGTH)(inputs)\n",
        "# The second argument to Model should be the output tensor, not MAX_LENGTH\n",
        "gpt2 = Model(inputs, outputs)  # Changed MAX_LENGTH to outputs\n",
        "\n",
        "gpt2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "cNvce29-5u1c",
        "outputId": "a41aacec-272b-42a1-857d-295a8d8f12bf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'gpt2_9', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1824\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gpt2_9 (\u001b[38;5;33mGPT2\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1824\u001b[0m, \u001b[38;5;34m50257\u001b[0m)    │   \u001b[38;5;34m200,896,849\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1824</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gpt2_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1824</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)    │   <span style=\"color: #00af00; text-decoration-color: #00af00\">200,896,849</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m200,896,849\u001b[0m (766.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">200,896,849</span> (766.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m200,896,849\u001b[0m (766.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">200,896,849</span> (766.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}